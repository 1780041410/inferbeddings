% !TeX spellcheck = en_GB

\section{Background} \label{sec:background}

%
In this section we introduce Knowledge Graphs, and describe several models from the literature for learning continuous representations of the entities and relations they describe.
%

\subsection{Knowledge Graphs}

%
Knowledge Graphs represent information in the form of entities and relationships between them~\cite{DBLP:journals/pieee/Nickel0TG16}.
%
Entities can be anything, including persons, documents, physical objects and abstract concepts.
%
In the following, we assume the Knowledge Graph follows the \emph{Resource Description Framework} (RDF)~\cite{Wood:14:RCA} data model: RDF is a W3C recommended framework for representing information about entities, also referred to as \emph{resources}.
%

%
Let $\ents$ and $\rels$ represent a set of entities and a set of relations, respectively.
%
A RDF Knowledge Base, also referred to as \emph{RDF graph}, is defined as a set of $\spo \in \ents \times \rels \times \ents$ triples, each consisting of a subject $\rs \in \ents$, a predicate $\rp \in \rels$ and an object $\ro \in \ents$: $\rs$ and $\ro$ are entities, while $\rp$ is a relation type.
%
Each $\spo$ triple encodes the statement ``$\rs$ has a relationship $\rp$ with $\ro$'', which can be encoded in predicate logic by the formula $\rp(\rs, \ro)$.
%

%
An RDF graph can be represented as a labeled directed multi-graph, in which each triple is represented as an edge connecting two nodes: source and target nodes represent the subject and the object of the triple, and the edge label represents the predicate.
%
Note that RDF and Knowledge Graphs in general adhere to the \emph{Open World Assumption}~\cite{Hayes:14:RS}: a triple not in the graph does not imply that the corresponding statement is false, but rather that its truth value is \emph{unknown}, \ie it cannot be known from the RDF graph.
%

\begin{example} \label{ex:rdf}
%
Consider the following statement: \emph{``Rammstein is a German music band, and KMFDM is also a music band.''}
%
It can be expressed by the following RDF triples:
%
 \begin{center}
  \begin{tabular}{L{2.5cm}C{2cm}R{2.5cm}}
   \toprule
   \textbf{Subject} & \textbf{Predicate} &  \textbf{Object} \\
   \midrule
   $\langle$ \res{Rammstein}, & \res{type}, & \res{Music Band} $\rangle$ \\
   $\langle$ \res{Rammstein}, & \res{nationality}, & \res{German} $\rangle$ \\
   $\langle$ \res{KMFDM}, & \res{type}, & \res{Music Band} $\rangle$ \\
   \bottomrule
  \end{tabular}
 \end{center}
%
The fact that the triple $\langle \res{KMFDM}, \res{nationality}, \res{German} \rangle$ is not present in the Knowledge Graph does not imply that KMFDM is not German, but rather that we do not know whether KMFDM is German or not.
%
\end{example}
%
In the following, we denote by $\tspace \triangleq \ents \times \rels \times \ents$ the set of \emph{possible triples} that can be represented by using the entities and relations in a Knowledge Graph, each representing a distinct $\langle \rs, \rp, \ro \rangle$ statement.
%

\subsection{Knowledge Graph Embeddings}

%
In Knowledge Graph embedding models, the link prediction score for each $\spo$ triple (fact) is defined as a function of the distributed representations -- or \emph{embeddings} -- associated with the subject $\rs$, the predicate $\rp$ and the object $\ro$ of the triple.
%
Each different model is characterized by its scoring function $\fscore(\Cdot ; \params)$, where the model parameters $\params$ correspond to the embedding vectors of all entities and relations in the graph.
%

%
Several models have been proposed for solving the link prediction problem.
%
Three largely popular models, due to their effectiveness, are the Translating Embeddings model~\cite{DBLP:conf/nips/BordesUGWY13}, the Bilinear-Diagonal model~\cite{yang15:embedding} and, more recently, the Complex Embeddings model~\cite{DBLP:conf/icml/TrouillonWRGB16}.
%
Such models can scale to very large and Web-scale Knowledge Graphs, thanks to:
%
\begin{inparaenum}[1)]
%
 \item A space complexity that grows \emph{linearly} with the number of entities and relations in the Knowledge Graph; and
%
 \item Efficient and scalable scoring functions and parameters learning procedures.
%
\end{inparaenum}
%

%
\subsubsection{The Translating Embeddings Model}
%

%
In the Translating Embeddings model (or \mdl{TransE}), each entity $e \in \ents$ is associated with a unique continuous \emph{embedding vector} $\emb{e} \in \Real^{k}$, where $k \in \Natural$ is a user-defined hyper-parameter.
%
Each vector $\emb{e}$ can be interpreted as a collection of continuous \emph{latent features} describing $e$~\cite{DBLP:journals/pieee/Nickel0TG16}.
%
Similarly, each predicate $\rp \in \rels$ is associated with a unique continuous embedding vector $\remb{\rp} \in \Real^{k}$.
%

%
Given a triple $\spo$, it prediction score $\fscore_{\rs\rp\ro}^{\mdl{TransE}}$ is defined by the embedding vectors $\emb{\rs}, \remb{\rp}, \emb{\ro} \in \Real^{k}$, respectively associated with the subject $\rs$, the predicate $\rp$ and the object $\ro$ of the triple.
%
Specifically, the score for a triple $\spo$ is defined as follows:
%
\begin{equation*} \label{eq:transe}
 \begin{aligned}
 %
  \fscore_{\mdl{TransE}}(\spo ; \params) & \triangleq - \norm{\emb{\rs} + \remb{\rp} - \emb{\ro}},
 %
 \end{aligned}
\end{equation*}
%
\noindent where $\norm{\Cdot}$ denotes either the $L_{1}$ or the $L_{2}$ norm, and $\norm{\mathbf{x}_{1} - \mathbf{x}_{2}}$ 
denotes the distance between vectors $\mathbf{x}_{1}$ and $\mathbf{x}_{2}$.
%

%
\subsubsection{The Bilinear-Diagonal Model}
%

%
The Bilinear-Diagonal model (or \mdl{DistMult}) is a variant of \mdl{TransE}, where:
%
\begin{inparaenum}[1)]
%
 \item The predicate embedding defines a \emph{scaling} operation, and
%
 \item The \emph{dot product} is used for assessing the similarity between two vectors.
%
\end{inparaenum}
%
In this model, the score of a triple $\spo$ is defined as follows:
%
\begin{equation*} \label{eq:distmult}
\begin{aligned}
\fscore_{\mdl{DistMult}}(\spo ; \params) & \triangleq \tdot{\remb{\rp}}{\emb{\rs}}{\emb{\ro}},
\end{aligned}
\end{equation*}
%
\noindent where, given $\mathbf{x}_{1}, \mathbf{x}_{2}, \mathbf{x}_{3} \in \Real^{k}$, $\tdot{\mathbf{x}_{1}}{\mathbf{x}_{2}}{\mathbf{x}_{3}} \triangleq \sum_{i=1}^{k} \mathbf{x}_{1, i} \mathbf{x}_{2, i} \mathbf{x}_{3, i}$ is the standard component-wise multi-linear dot product.
%

%
\subsubsection{The Complex Embeddings Model}
%

%
The Complex Embeddings model (or \mdl{ComplEx}) is related to \mdl{DistMult}, but uses complex-valued embeddings while retaining the mathematical definition of the dot product.
%
In this model, the score of a triple $\spo$ is defined as follows:
%
\begin{equation*}
%
 \begin{aligned}
%
  \fscore_{\mdl{ComplEx}}(\spo ; \params) & \triangleq \ReP{\tdot{\remb{\rp}}{\emb{\rs}}{\conjt{\emb{\ro}}}} \\
%
  & = \tdot{\ReP{\remb{\rp}}}{\ReP{\emb{\rs}}}{\ReP{\emb{\ro}}} \\
  & \qquad + \tdot{\ReP{\remb{\rp}}}{\ImP{\emb{\rs}}}{\ImP{\emb{\ro}}} \\
  & \qquad + \tdot{\ImP{\remb{\rp}}}{\ReP{\emb{\rs}}}{\ImP{\emb{\ro}}} \\
  & \qquad - \tdot{\ImP{\remb{\rp}}}{\ImP{\emb{\rs}}}{\ReP{\emb{\ro}}}, \\
%
 \end{aligned}
\end{equation*}
%
\noindent where $\remb{\rp}, \emb{\rs}, \emb{\ro} \in \Complex^{k}$ are complex vectors, $\conjt{\mathbf{x}}$ denotes the complex conjugate of $\mathbf{x}$~\footnote{Given $x \in \Complex$, its complex conjugate is $\conjt{x} \triangleq \ReP{x} - i \ImP{x}$.}, while 
$\ReP{\mathbf{x}} \in \Real^{k}$ and $\ImP{\mathbf{x}} \in \Real^{k}$ denote the real part and the imaginary part of $\mathbf{x}$, respectively.
%

%
\subsubsection{Learning the Model Parameters}
%

%
In \cite{DBLP:conf/nips/BordesUGWY13,yang15:embedding,DBLP:journals/pieee/Nickel0TG16}, authors estimate the model parameters by minimizing the a ranking loss, where negative examples are obtained by \emph{corrupting} the triples in the Knowledge Graph.
%
More formally, given a triple $\spo$, negative examples are generated by the corruption process defined by $\corr{\Cdot}$:
%
\begin{equation} \label{eq:corruption}
%
 \corr{\spo} \triangleq \{ \langle \tilde{\rs}, \rp, \ro \rangle \mid \tilde{\rs} \in \ents \} \cup \{ \langle \rs, \rp, \tilde{\ro} \rangle \mid \tilde{\ro} \in \ents \},
%
\end{equation}
%
\noindent \ie given a triple, the process generates a set of triples by replacing its subject and object with all other entities in $\KG$.
%
This method of sampling negative examples is motivated by the \emph{Local Closed World Assumption} (LCWA)~\cite{DBLP:conf/kdd/0001GHHLMSSZ14}: if a triple $\spo$ exists in the graph, other triples obtained by corrupting either the subject or the object of the triples not appearing in the graph can be considered as negative examples.
%
%
%
The ranking loss is then defined as follows:
%
\begin{equation} \label{eq:loss}
%
 \loss(\params) \triangleq \sum_{t \in \KG} \sum_{\tilde{t} \in \corr{t}} \hinge{\gamma - \fscore(t ; \params) + \fscore(\tilde{t} ; \params)},
%
\end{equation}
%
\noindent where $\hinge{x} \triangleq \max \{ 0, x \}$.
%
Note that the loss function in Eq.~\ref{eq:loss} will reach its global minimum $0$ iff, for each pair of positive and negative examples $t$ and $\tilde{t}$, the score of the (true) triple $t$ is higher with a margin of at least $\gamma$ than the score of the (missing) triple $\tilde{t}$.
%
The optimal parameters can be learned by solving the following minimization problem:
%
\begin{equation} \label{eq:min}
%
 \begin{aligned}
  & \underset{\params}{\text{minimize}}
  & & \loss(\params) \\
  & \text{subject to}
  & & \displaystyle{\forall e \in \ents: \; \norm{\emb{e}} = 1.}
 \end{aligned}
%
\end{equation}
%
The norm constraints on the entity embeddings prevent to trivially solve the optimization problem by increasing the norm of the embedding vectors.
%