% !TeX spellcheck = en_GB

\section{Introduction} \label{sec:introduction}

%
Knowledge Graphs are graph-structured Knowledge Bases, where facts are represented in the form of relationships between entities: they are powerful instruments in search, analytics and data integration.
%
Developments in the area of Knowledge Graphs were initially driven by academic efforts, such as \kg{DBpedia}~\cite{DBLP:conf/semweb/AuerBKLCI07}, \kg{Freebase}~\cite{DBLP:conf/aaai/BollackerCT07}, \kg{YAGO}~\cite{DBLP:conf/www/SuchanekKW07} and \kg{NELL}~\cite{DBLP:conf/aaai/CarlsonBKSHM10}.
%
At the time of this writing, Knowledge Graphs are also widely popular in industry applications, such as the \kg{Freebase}-based Google's Knowledge Graph~\footnote{\url{https://developers.google.com/knowledge-graph/}} and Knowledge Vault~\cite{DBLP:conf/kdd/0001GHHLMSSZ14} projects, which support the company's search and smart assistance services.
%

%
However, despite the engineering efforts, real world Knowledge Graphs are often far from being complete.
%
For instance, consider \kg{Freebase}: 71\% of the persons described in the knowledge base have no known place of birth, 75\% have no known nationality, and coverage for less frequent relations can be even lower~\cite{DBLP:conf/kdd/0001GHHLMSSZ14}.
%
Similarly, in \kg{DBpedia} 66\% of the persons have no known place of birth, while 58\% of the scientists are missing a fact stating what they are known for~\cite{DBLP:conf/semweb/KrompassBT15}.
%

%
Therefore, in this paper we focus on the problem of \emph{predicting missing links} in Knowledge Graphs, so to discover new facts: in the literature, this problem is also known as \emph{link prediction} and \emph{Knowledge Base completion}.
%
Current successful link prediction methods heavily rely on learned distributed vector representations of entities and relations in the Knowledge Graph~\cite{DBLP:conf/www/NickelTK12,DBLP:conf/nips/BordesUGWY13,DBLP:conf/naacl/RiedelYMM13,yang15:embedding,DBLP:conf/icml/TrouillonWRGB16}.
%
we refer to \cite{DBLP:journals/pieee/Nickel0TG16} for a detailed description of such methods.
%
Although such models are able to learn robust representations of entities and relations from large amount of data, they lack common-sense knowledge and reasoning.
%

\subsection{Related Works}

%
Combining neural methods with symbolic common-sense knowledge, \eg in the form of implication rules, is an actively researched area~\cite{DBLP:conf/cikm/WangMC14,DBLP:conf/ijcai/WangWG15,DBLP:journals/corr/VendrovKFU15,DBLP:conf/akbc/RocktaschelR16,DBLP:conf/emnlp/DemeesterRR16}
%
In \cite{DBLP:conf/naacl/RocktaschelSR15} authors regularize entity-tuple and relation embeddings via First-Order Logic rules: every rule is propositionalized, and a differentiable loss term is added for every propositional rule.
%
However, this approach does not scale to large knowledge bases: even a simple rule such as
%
\begin{equation*}
 \begin{aligned}
  \forall x, y, z: \rel{siblingOf}(x, y) \; \land \; \rel{parentOf}(y, z) \\ \implies \rel{uncleOf}(x, z)
 \end{aligned}
\end{equation*}
%
\noindent would result in a very large number of loss terms.
%
In \cite{DBLP:conf/emnlp/DemeesterRR16} authors overcome this problem by minimizing an upper bound of the loss that encourages the implication between relations to hold, entirely independent from the number of entities.
%
However, their approach -- a variant \mdl{Model F}~\cite{DBLP:conf/RiedelYMM13} -- is only able to model a very restricted set of rules in the form:
%
\[ \forall x, y : \rp(x, y) \implies \relq(x, y), \]
%
\noindent where $\rp$ and $\relq$ are two arbitrary relations.
%

\subsection{Contribution}

%
\emph{Adversarial examples} are examples crafted for significantly increasing the loss functional of a machine learning model, while \emph{adversarial training} is the process of training a model to correctly classify both training and adversarial examples~\cite{szegedy14:intriguing,goodfellow15:harnessing}.
%

%
In this paper we present a method, named \emph{Adversarial Rule Injection} (\ARI), for incorporating general First-Order Logic rules in Knowledge Graph embedding models.
%
\ARI relies on an \emph{adversarial training architecture}, where:
%
\begin{inparaenum}[1)]
%
 \item An \emph{adversary} finds counter-examples that maximize a rule violation loss, and
%
 \item A tunes the parameters so to minimize the rule violation loss on such counter-examples.
%
\end{inparaenum}
%

%
In particular, in \ARI, counter-examples are found in the \emph{continuous embedding space} rather than in the discrete fact space.
%
This has several advantages, namely:
%
\begin{description}
%
 \item[Scalability:] It decouples the complexity of the method from the number of entities in the Knowledge Base, which can be in the order of millions or more.
%
 \item[Generalizability:] It enforces rules to hold everywhere in the embedding space, even on previously unseen entity embeddings.
%
 \item[Flexibility:] It can be used jointly with any Knowledge Graph embedding model, without making any assumption on its architecture.
%
\end{description}
%

%
In Statistical Relational Learning~\cite{getoor:srlbook07}, the proposed approach falls in the category of \emph{lifted inference and learning}~\cite{DBLP:conf/ijcai/Poole03,DBLP:conf/ijcai/BrazAR05}: by imposing the desired constraints on the whole entity embedding space, we can efficiently reason about groups of objects as a whole by exploiting symmetries in the relational structure of the model.
%
This allows imposing a large number of First-Order rules while learning the distributed representations of all entities and relations in the Knowledge Graph.
%
Two fundamental advantages with respect to the method in \cite{DBLP:conf/naacl/RocktaschelSR15} are the drastically lower computation times and the fact that rules are enforced to hold on the whole embedding space, even on previously unseen entity embeddings -- thus covering the cases where not all entity embeddings are visible at training time, such as in Row-Less Universal Schema~\cite{DBLP:journals/corr/VergaM16}.
%
The method in \cite{DBLP:conf/emnlp/DemeesterRR16} is also \emph{lifted}, but can only handle a very small subset of First-Order rules.
%
