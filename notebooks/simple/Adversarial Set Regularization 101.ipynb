{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def read_triples(path):\n",
    "    triples = []\n",
    "    with open(path, 'rt') as f:\n",
    "        for line in f.readlines():\n",
    "            s, p, o = line.split()\n",
    "            triples += [(s.strip(), p.strip(), o.strip())]\n",
    "    return triples\n",
    "\n",
    "\n",
    "def unit_cube_projection(var_matrix):\n",
    "    unit_cube_projection = tf.minimum(1., tf.maximum(var_matrix, 0.))\n",
    "    return tf.assign(var_matrix, unit_cube_projection)\n",
    "\n",
    "\n",
    "def make_batches(size, batch_size):\n",
    "    nb_batch = int(np.ceil(size / float(batch_size)))\n",
    "    res = [(i * batch_size, min(size, (i + 1) * batch_size)) for i in range(0, nb_batch)]\n",
    "    return res\n",
    "\n",
    "class IndexGenerator:\n",
    "    def __init__(self):\n",
    "        self.random_state = np.random.RandomState(0)\n",
    "\n",
    "    def __call__(self, n_samples, candidate_indices):\n",
    "        shuffled_indices = candidate_indices[self.random_state.permutation(len(candidate_indices))]\n",
    "        rand_ints = shuffled_indices[np.arange(n_samples) % len(shuffled_indices)]\n",
    "        return rand_ints\n",
    "\n",
    "class DistMult:\n",
    "    def __init__(self, subject_embeddings=None, object_embeddings=None,\n",
    "                 predicate_embeddings=None,):\n",
    "        self.subject_embeddings, self.object_embeddings = subject_embeddings, object_embeddings\n",
    "        self.predicate_embeddings = predicate_embeddings\n",
    "\n",
    "    def __call__(self):\n",
    "        scores = tf.reduce_sum(self.subject_embeddings *\n",
    "                               self.predicate_embeddings *\n",
    "                               self.object_embeddings, axis=1)\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_embedding_size = 150\n",
    "predicate_embedding_size = 150\n",
    "\n",
    "seed = 0\n",
    "margin = 5\n",
    "\n",
    "nb_epochs = 1000\n",
    "nb_batches = 10\n",
    "\n",
    "np.random.seed(seed)\n",
    "random_state = np.random.RandomState(seed)\n",
    "tf.set_random_seed(seed)\n",
    "\n",
    "dataset_name = 'fb122'\n",
    "\n",
    "train_triples = read_triples('{}/{}.train.tsv'.format(dataset_name, dataset_name))\n",
    "valid_triples = read_triples('{}/{}.valid.tsv'.format(dataset_name, dataset_name))\n",
    "test_triples = read_triples('{}/{}.test.tsv'.format(dataset_name, dataset_name))\n",
    "\n",
    "from parse import parse_clause\n",
    "with open('{}/{}-clauses.pl'.format(dataset_name, dataset_name), 'rt') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "clauses = [parse_clause(line.strip()) for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_triples = train_triples + valid_triples + test_triples\n",
    "\n",
    "entity_set = {s for (s, p, o) in all_triples} | {o for (s, p, o) in all_triples}\n",
    "predicate_set = {p for (s, p, o) in all_triples}\n",
    "\n",
    "nb_entities, nb_predicates = len(entity_set), len(predicate_set)\n",
    "nb_examples = len(train_triples)\n",
    "\n",
    "entity_to_idx = {entity: idx for idx, entity in enumerate(sorted(entity_set))}\n",
    "predicate_to_idx = {predicate: idx for idx, predicate in enumerate(sorted(predicate_set))}\n",
    "\n",
    "entity_embedding_layer = tf.get_variable('entities', shape=[nb_entities, entity_embedding_size],\n",
    "                                         initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "predicate_embedding_layer = tf.get_variable('predicates', shape=[nb_predicates, predicate_embedding_size],\n",
    "                                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "subject_inputs = tf.placeholder(tf.int32, shape=[None])\n",
    "predicate_inputs = tf.placeholder(tf.int32, shape=[None])\n",
    "object_inputs = tf.placeholder(tf.int32, shape=[None])\n",
    "\n",
    "target_inputs = tf.placeholder(tf.float32, shape=[None])\n",
    "\n",
    "subject_embeddings = tf.nn.embedding_lookup(entity_embedding_layer, subject_inputs)\n",
    "predicate_embeddings = tf.nn.embedding_lookup(predicate_embedding_layer, predicate_inputs)\n",
    "object_embeddings = tf.nn.embedding_lookup(entity_embedding_layer, object_inputs)\n",
    "\n",
    "model_parameters = {\n",
    "    'subject_embeddings': subject_embeddings,\n",
    "    'predicate_embeddings': predicate_embeddings,\n",
    "    'object_embeddings': object_embeddings\n",
    "}\n",
    "\n",
    "model_class = DistMult\n",
    "model = model_class(**model_parameters)\n",
    "\n",
    "scores = model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Adversary:\n",
    "    \"\"\"\n",
    "    Utility class for, given a set of clauses, computing the symbolic violation loss.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, clauses, predicate_to_index,\n",
    "                 entity_embedding_layer, predicate_embedding_layer,\n",
    "                 model_class, model_parameters, loss_function=None, batch_size=1):\n",
    "\n",
    "        self.clauses, self.predicate_to_index = clauses, predicate_to_index\n",
    "        self.entity_embedding_layer = entity_embedding_layer\n",
    "        self.predicate_embedding_layer = predicate_embedding_layer\n",
    "\n",
    "        self.entity_embedding_size = self.entity_embedding_layer.get_shape()[-1].value\n",
    "\n",
    "        self.model_class, self.model_parameters = model_class, model_parameters\n",
    "\n",
    "        self.pooling = pooling\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        def _violation_losses(body_scores, head_scores, margin):\n",
    "            _losses = tf.nn.relu(margin - head_scores + body_scores)\n",
    "            return tf.reduce_max(_losses)\n",
    "\n",
    "        self.loss_function = lambda body_scores, head_scores:\\\n",
    "            _violation_losses(body_scores, head_scores, margin=loss_margin)\n",
    "\n",
    "        # Symbolic functions computing the continuous loss\n",
    "        self.loss = tf.constant(.0)\n",
    "\n",
    "        # Trainable parameters of the adversarial model\n",
    "        self.parameters = []\n",
    "\n",
    "        # Mapping {clause:v2l} where \"clause\" is a clause, and v2l is a {var_name:layer} mapping\n",
    "        self.clause_to_variable_name_to_layer = dict()\n",
    "        self.clause_to_loss = dict()\n",
    "\n",
    "        for clause_idx, clause in enumerate(clauses):\n",
    "            clause_loss, clause_parameters, variable_name_to_layer =\\\n",
    "                self._parse_clause('clause_{}'.format(clause_idx), clause)\n",
    "\n",
    "            self.clause_to_variable_name_to_layer[clause] = variable_name_to_layer\n",
    "            self.clause_to_loss[clause] = clause_loss\n",
    "\n",
    "            self.loss += clause_loss\n",
    "            self.parameters += clause_parameters\n",
    "\n",
    "    def _parse_atom(self, atom, variable_name_to_layer):\n",
    "        \"\"\"\n",
    "        Given an atom in the form p(X, Y), where X and Y are associated to two distinct [1, k] embedding layers,\n",
    "        return the symbolic score of the atom.\n",
    "        \"\"\"\n",
    "        predicate_idx = self.predicate_to_index[atom.predicate.name]\n",
    "        \n",
    "        # [batch_size x 1 x embedding_size] tensor\n",
    "        predicate_embeddings = tf.nn.embedding_lookup(self.predicate_embedding_layer, [[predicate_idx]] * self.batch_size)\n",
    "        arg1_name, arg2_name = atom.arguments[0].name, atom.arguments[1].name\n",
    "\n",
    "        # [batch_size x embedding_size] variables\n",
    "        arg1_layer, arg2_layer = variable_name_to_layer[arg1_name], variable_name_to_layer[arg2_name]\n",
    "\n",
    "        subject_embeddings = variable_name_to_layer[arg1_name]\n",
    "        object_embeddings = variable_name_to_layer[arg2_name]\n",
    "\n",
    "        model_parameters = self.model_parameters\n",
    "        \n",
    "        model_parameters['subject_embeddings'] = subject_embeddings\n",
    "        model_parameters['object_embeddings'] = object_embeddings\n",
    "        \n",
    "        model_parameters['predicate_embeddings'] = predicate_embeddings\n",
    "\n",
    "        scoring_model = self.model_class(**model_parameters)\n",
    "        atom_score = scoring_model()\n",
    "\n",
    "        return atom_score\n",
    "\n",
    "    def _parse_conjunction(self, atoms, variable_name_to_layer):\n",
    "        \"\"\"\n",
    "        Given a conjunction of atoms in the form p(X0, X1), q(X2, X3), r(X4, X5), return its symbolic score.\n",
    "        \"\"\"\n",
    "        conjunction_score = None\n",
    "        for atom in atoms:\n",
    "            atom_score = self._parse_atom(atom, variable_name_to_layer=variable_name_to_layer)\n",
    "            conjunction_score = atom_score if conjunction_score is None else tf.minimum(conjunction_score, atom_score)\n",
    "        return conjunction_score\n",
    "\n",
    "    def _parse_clause(self, name, clause):\n",
    "        \"\"\"\n",
    "        Given a clause in the form p(X0, X1) :- q(X2, X3), r(X4, X5), return its symbolic score.\n",
    "        \"\"\"\n",
    "        head, body = clause.head, clause.body\n",
    "\n",
    "        # Enumerate all variables\n",
    "        variable_names = {argument.name for argument in head.arguments}\n",
    "        for body_atom in body:\n",
    "            variable_names |= {argument.name for argument in body_atom.arguments}\n",
    "\n",
    "        # Instantiate a new layer for each variable\n",
    "        variable_name_to_layer = dict()\n",
    "        for variable_name in sorted(variable_names):\n",
    "            # [batch_size, embedding_size] variable\n",
    "            variable_layer = tf.get_variable('{}_{}_violator'.format(name, variable_name),\n",
    "                                             shape=[self.batch_size, self.entity_embedding_size],\n",
    "                                             initializer=tf.contrib.layers.xavier_initializer())\n",
    "            variable_name_to_layer[variable_name] = variable_layer\n",
    "\n",
    "        head_score = self._parse_atom(head, variable_name_to_layer=variable_name_to_layer)\n",
    "        body_score = self._parse_conjunction(body, variable_name_to_layer=variable_name_to_layer)\n",
    "\n",
    "        parameters = [variable_name_to_layer[variable_name] for variable_name in sorted(variable_names)]\n",
    "        loss = self.loss_function(body_score, head_score)\n",
    "        return loss, parameters, variable_name_to_layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adversary = Adversary(clauses=clauses, predicate_to_index=predicate_to_idx,\n",
    "                      entity_embedding_layer=entity_embedding_layer,\n",
    "                      predicate_embedding_layer=predicate_embedding_layer,\n",
    "                      model_class=model_class, model_parameters=model_parameters)\n",
    "violation_loss = adversary.loss\n",
    "\n",
    "ADVERSARIAL_OPTIMIZER_SCOPE_NAME = 'adversary/optimizer'\n",
    "with tf.variable_scope(ADVERSARIAL_OPTIMIZER_SCOPE_NAME):\n",
    "    adversarial_optimizer = tf.train.AdagradOptimizer(learning_rate=0.1)\n",
    "    adversarial_training_step = adversarial_optimizer.minimize(- violation_loss, var_list=adversary.parameters)\n",
    "\n",
    "\n",
    "hinge_losses = tf.nn.relu(margin - scores * (2 * target_inputs - 1))\n",
    "loss = tf.reduce_sum(hinge_losses)\n",
    "\n",
    "optimizer = tf.train.AdagradOptimizer(learning_rate=0.1)\n",
    "training_step = optimizer.minimize(loss)\n",
    "\n",
    "projection_step = unit_cube_projection(entity_embedding_layer)\n",
    "    \n",
    "\n",
    "import math\n",
    "batch_size = math.ceil(nb_examples / nb_batches)\n",
    "batches = make_batches(nb_examples, batch_size)\n",
    "\n",
    "nb_versions = 3\n",
    "\n",
    "Xs = np.array([entity_to_idx[s] for (s, p, o) in train_triples], dtype=np.int32)\n",
    "Xp = np.array([predicate_to_idx[p] for (s, p, o) in train_triples], dtype=np.int32)\n",
    "Xo = np.array([entity_to_idx[o] for (s, p, o) in train_triples], dtype=np.int32)\n",
    "\n",
    "index_gen = IndexGenerator()\n",
    "\n",
    "init_op = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Epoch 10\tLoss value: 4.7656 ± 0.0611\n",
      "INFO:__main__:Epoch 20\tLoss value: 2.3160 ± 0.0437\n",
      "INFO:__main__:Epoch 30\tLoss value: 1.5321 ± 0.0207\n",
      "INFO:__main__:Epoch 40\tLoss value: 1.3006 ± 0.0390\n",
      "INFO:__main__:Epoch 50\tLoss value: 1.1647 ± 0.0286\n",
      "INFO:__main__:Epoch 60\tLoss value: 1.0867 ± 0.0268\n",
      "INFO:__main__:Epoch 70\tLoss value: 1.0314 ± 0.0329\n",
      "INFO:__main__:Epoch 80\tLoss value: 0.9912 ± 0.0414\n",
      "INFO:__main__:Epoch 90\tLoss value: 0.9435 ± 0.0478\n",
      "INFO:__main__:Epoch 100\tLoss value: 0.9130 ± 0.0263\n",
      "INFO:__main__:Epoch 110\tLoss value: 0.9040 ± 0.0190\n",
      "INFO:__main__:Epoch 120\tLoss value: 0.8928 ± 0.0215\n",
      "INFO:__main__:Epoch 130\tLoss value: 0.8715 ± 0.0250\n",
      "INFO:__main__:Epoch 140\tLoss value: 0.8521 ± 0.0309\n",
      "INFO:__main__:Epoch 150\tLoss value: 0.8521 ± 0.0145\n",
      "INFO:__main__:Epoch 160\tLoss value: 0.8340 ± 0.0300\n",
      "INFO:__main__:Epoch 170\tLoss value: 0.8322 ± 0.0248\n",
      "INFO:__main__:Epoch 180\tLoss value: 0.8180 ± 0.0346\n",
      "INFO:__main__:Epoch 190\tLoss value: 0.7923 ± 0.0238\n",
      "INFO:__main__:Epoch 200\tLoss value: 0.8090 ± 0.0245\n",
      "INFO:__main__:Epoch 210\tLoss value: 0.8137 ± 0.0243\n",
      "INFO:__main__:Epoch 220\tLoss value: 0.7940 ± 0.0168\n",
      "INFO:__main__:Epoch 230\tLoss value: 0.7875 ± 0.0115\n",
      "INFO:__main__:Epoch 240\tLoss value: 0.7863 ± 0.0239\n",
      "INFO:__main__:Epoch 250\tLoss value: 0.7696 ± 0.0210\n",
      "INFO:__main__:Epoch 260\tLoss value: 0.7712 ± 0.0336\n",
      "INFO:__main__:Epoch 270\tLoss value: 0.7722 ± 0.0355\n",
      "INFO:__main__:Epoch 280\tLoss value: 0.7608 ± 0.0084\n",
      "INFO:__main__:Epoch 290\tLoss value: 0.7659 ± 0.0283\n",
      "INFO:__main__:Epoch 300\tLoss value: 0.7561 ± 0.0218\n",
      "INFO:__main__:Epoch 310\tLoss value: 0.7477 ± 0.0338\n",
      "INFO:__main__:Epoch 320\tLoss value: 0.7457 ± 0.0171\n",
      "INFO:__main__:Epoch 330\tLoss value: 0.7429 ± 0.0226\n",
      "INFO:__main__:Epoch 340\tLoss value: 0.7370 ± 0.0290\n",
      "INFO:__main__:Epoch 350\tLoss value: 0.7546 ± 0.0157\n",
      "INFO:__main__:Epoch 360\tLoss value: 0.7315 ± 0.0196\n",
      "INFO:__main__:Epoch 370\tLoss value: 0.7300 ± 0.0232\n",
      "INFO:__main__:Epoch 380\tLoss value: 0.7419 ± 0.0253\n",
      "INFO:__main__:Epoch 390\tLoss value: 0.7366 ± 0.0238\n",
      "INFO:__main__:Epoch 400\tLoss value: 0.7083 ± 0.0294\n",
      "INFO:__main__:Epoch 410\tLoss value: 0.7333 ± 0.0171\n",
      "INFO:__main__:Epoch 420\tLoss value: 0.7248 ± 0.0309\n",
      "INFO:__main__:Epoch 430\tLoss value: 0.7171 ± 0.0248\n",
      "INFO:__main__:Epoch 440\tLoss value: 0.7217 ± 0.0189\n",
      "INFO:__main__:Epoch 450\tLoss value: 0.7239 ± 0.0304\n",
      "INFO:__main__:Epoch 460\tLoss value: 0.7210 ± 0.0341\n",
      "INFO:__main__:Epoch 470\tLoss value: 0.7219 ± 0.0424\n",
      "INFO:__main__:Epoch 480\tLoss value: 0.7092 ± 0.0221\n",
      "INFO:__main__:Epoch 490\tLoss value: 0.7340 ± 0.0222\n",
      "INFO:__main__:Epoch 500\tLoss value: 0.7216 ± 0.0312\n",
      "INFO:__main__:Epoch 510\tLoss value: 0.7025 ± 0.0359\n",
      "INFO:__main__:Epoch 520\tLoss value: 0.7003 ± 0.0225\n",
      "INFO:__main__:Epoch 530\tLoss value: 0.7040 ± 0.0209\n",
      "INFO:__main__:Epoch 540\tLoss value: 0.7121 ± 0.0185\n",
      "INFO:__main__:Epoch 550\tLoss value: 0.7138 ± 0.0230\n",
      "INFO:__main__:Epoch 560\tLoss value: 0.7085 ± 0.0170\n",
      "INFO:__main__:Epoch 570\tLoss value: 0.7207 ± 0.0216\n",
      "INFO:__main__:Epoch 580\tLoss value: 0.7028 ± 0.0217\n",
      "INFO:__main__:Epoch 590\tLoss value: 0.7032 ± 0.0311\n",
      "INFO:__main__:Epoch 600\tLoss value: 0.7176 ± 0.0205\n",
      "INFO:__main__:Epoch 610\tLoss value: 0.7091 ± 0.0266\n",
      "INFO:__main__:Epoch 620\tLoss value: 0.6993 ± 0.0211\n",
      "INFO:__main__:Epoch 630\tLoss value: 0.7090 ± 0.0244\n",
      "INFO:__main__:Epoch 640\tLoss value: 0.6971 ± 0.0261\n",
      "INFO:__main__:Epoch 650\tLoss value: 0.7090 ± 0.0188\n",
      "INFO:__main__:Epoch 660\tLoss value: 0.6993 ± 0.0290\n",
      "INFO:__main__:Epoch 670\tLoss value: 0.7022 ± 0.0178\n",
      "INFO:__main__:Epoch 680\tLoss value: 0.6869 ± 0.0317\n",
      "INFO:__main__:Epoch 690\tLoss value: 0.6996 ± 0.0237\n",
      "INFO:__main__:Epoch 700\tLoss value: 0.6957 ± 0.0290\n",
      "INFO:__main__:Epoch 710\tLoss value: 0.6973 ± 0.0243\n",
      "INFO:__main__:Epoch 720\tLoss value: 0.6942 ± 0.0220\n",
      "INFO:__main__:Epoch 730\tLoss value: 0.6973 ± 0.0260\n",
      "INFO:__main__:Epoch 740\tLoss value: 0.6891 ± 0.0249\n",
      "INFO:__main__:Epoch 750\tLoss value: 0.6966 ± 0.0284\n",
      "INFO:__main__:Epoch 760\tLoss value: 0.6829 ± 0.0190\n",
      "INFO:__main__:Epoch 770\tLoss value: 0.7045 ± 0.0246\n",
      "INFO:__main__:Epoch 780\tLoss value: 0.6901 ± 0.0309\n",
      "INFO:__main__:Epoch 790\tLoss value: 0.6956 ± 0.0222\n",
      "INFO:__main__:Epoch 800\tLoss value: 0.6990 ± 0.0249\n",
      "INFO:__main__:Epoch 810\tLoss value: 0.6869 ± 0.0122\n",
      "INFO:__main__:Epoch 820\tLoss value: 0.6977 ± 0.0369\n",
      "INFO:__main__:Epoch 830\tLoss value: 0.6944 ± 0.0138\n",
      "INFO:__main__:Epoch 840\tLoss value: 0.6875 ± 0.0277\n",
      "INFO:__main__:Epoch 850\tLoss value: 0.7041 ± 0.0272\n",
      "INFO:__main__:Epoch 860\tLoss value: 0.6857 ± 0.0234\n",
      "INFO:__main__:Epoch 870\tLoss value: 0.6871 ± 0.0210\n",
      "INFO:__main__:Epoch 880\tLoss value: 0.6805 ± 0.0164\n",
      "INFO:__main__:Epoch 890\tLoss value: 0.6892 ± 0.0263\n",
      "INFO:__main__:Epoch 900\tLoss value: 0.6936 ± 0.0293\n",
      "INFO:__main__:Epoch 910\tLoss value: 0.7062 ± 0.0205\n",
      "INFO:__main__:Epoch 920\tLoss value: 0.6868 ± 0.0250\n",
      "INFO:__main__:Epoch 930\tLoss value: 0.6785 ± 0.0245\n",
      "INFO:__main__:Epoch 940\tLoss value: 0.6753 ± 0.0217\n",
      "INFO:__main__:Epoch 950\tLoss value: 0.6930 ± 0.0206\n",
      "INFO:__main__:Epoch 960\tLoss value: 0.6904 ± 0.0296\n",
      "INFO:__main__:Epoch 970\tLoss value: 0.6906 ± 0.0207\n",
      "INFO:__main__:Epoch 980\tLoss value: 0.6753 ± 0.0187\n",
      "INFO:__main__:Epoch 990\tLoss value: 0.6758 ± 0.0239\n",
      "INFO:__main__:Epoch 1000\tLoss value: 0.6771 ± 0.0246\n"
     ]
    }
   ],
   "source": [
    "def stats(values):\n",
    "    return '{0:.4f} ± {1:.4f}'.format(round(np.mean(values), 4), round(np.std(values), 4))\n",
    "\n",
    "session = tf.Session()\n",
    "session.run(init_op)\n",
    "\n",
    "for epoch in range(1, nb_epochs + 1):\n",
    "    order = random_state.permutation(nb_examples)\n",
    "    Xs_shuf, Xp_shuf, Xo_shuf = Xs[order], Xp[order], Xo[order]\n",
    "    \n",
    "    loss_values = []\n",
    "\n",
    "    for batch_no, (batch_start, batch_end) in enumerate(batches):\n",
    "        curr_batch_size = batch_end - batch_start\n",
    "\n",
    "        Xs_batch = np.zeros(curr_batch_size * nb_versions, dtype=Xs_shuf.dtype)\n",
    "        Xp_batch = np.zeros(curr_batch_size * nb_versions, dtype=Xp_shuf.dtype)\n",
    "        Xo_batch = np.zeros(curr_batch_size * nb_versions, dtype=Xo_shuf.dtype)\n",
    "\n",
    "        Xs_batch[0::nb_versions] = Xs_shuf[batch_start:batch_end]\n",
    "        Xp_batch[0::nb_versions] = Xp_shuf[batch_start:batch_end]\n",
    "        Xo_batch[0::nb_versions] = Xo_shuf[batch_start:batch_end]\n",
    "\n",
    "        # Xs_batch[1::nb_versions] needs to be corrupted\n",
    "        Xs_batch[1::nb_versions] = index_gen(curr_batch_size, np.arange(nb_entities))\n",
    "        Xp_batch[1::nb_versions] = Xp_shuf[batch_start:batch_end]\n",
    "        Xo_batch[1::nb_versions] = Xo_shuf[batch_start:batch_end]\n",
    "\n",
    "        # Xo_batch[2::nb_versions] needs to be corrupted\n",
    "        Xs_batch[2::nb_versions] = Xs_shuf[batch_start:batch_end]\n",
    "        Xp_batch[2::nb_versions] = Xp_shuf[batch_start:batch_end]\n",
    "        Xo_batch[2::nb_versions] = index_gen(curr_batch_size, np.arange(nb_entities))\n",
    "\n",
    "        feed_dict = {\n",
    "            subject_inputs: Xs_batch, predicate_inputs: Xp_batch, object_inputs: Xo_batch,\n",
    "            target_inputs: np.array([1.0, 0.0, 0.0] * curr_batch_size)\n",
    "        }\n",
    "\n",
    "        _, loss_value = session.run([training_step, loss], feed_dict=feed_dict)\n",
    "        session.run(projection_step)\n",
    "\n",
    "        loss_values += [loss_value / (Xp_batch.shape[0] / nb_versions)]\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        logger.info('Epoch {0}\\tLoss value: {1}'.format(epoch, stats(loss_values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:0/9595 ..\n",
      "INFO:__main__:1000/9595 ..\n",
      "INFO:__main__:2000/9595 ..\n",
      "INFO:__main__:3000/9595 ..\n",
      "INFO:__main__:4000/9595 ..\n",
      "INFO:__main__:5000/9595 ..\n",
      "INFO:__main__:6000/9595 ..\n",
      "INFO:__main__:7000/9595 ..\n",
      "INFO:__main__:8000/9595 ..\n",
      "INFO:__main__:9000/9595 ..\n",
      "INFO:__main__:[valid] Raw Mean Rank: 701.0453882230328\n",
      "INFO:__main__:[valid] Raw Hits@1: 19.640437727983326\n",
      "INFO:__main__:[valid] Raw Hits@3: 34.34601354872329\n",
      "INFO:__main__:[valid] Raw Hits@5: 42.73579989577905\n",
      "INFO:__main__:[valid] Raw Hits@10: 49.63001563314226\n",
      "INFO:__main__:[valid] Filtered Mean Rank: 385.5411672746222\n",
      "INFO:__main__:[valid] Filtered Hits@1: 54.366857738405415\n",
      "INFO:__main__:[valid] Filtered Hits@3: 65.34653465346535\n",
      "INFO:__main__:[valid] Filtered Hits@5: 68.45752996352267\n",
      "INFO:__main__:[valid] Filtered Hits@10: 71.25065138092756\n",
      "INFO:__main__:0/11243 ..\n",
      "INFO:__main__:1000/11243 ..\n",
      "INFO:__main__:2000/11243 ..\n",
      "INFO:__main__:3000/11243 ..\n",
      "INFO:__main__:4000/11243 ..\n",
      "INFO:__main__:5000/11243 ..\n",
      "INFO:__main__:6000/11243 ..\n",
      "INFO:__main__:7000/11243 ..\n",
      "INFO:__main__:8000/11243 ..\n",
      "INFO:__main__:9000/11243 ..\n",
      "INFO:__main__:10000/11243 ..\n",
      "INFO:__main__:11000/11243 ..\n",
      "INFO:__main__:[test] Raw Mean Rank: 703.0659966201192\n",
      "INFO:__main__:[test] Raw Hits@1: 20.4883038334964\n",
      "INFO:__main__:[test] Raw Hits@3: 35.2441519167482\n",
      "INFO:__main__:[test] Raw Hits@5: 43.19131904296006\n",
      "INFO:__main__:[test] Raw Hits@10: 49.75540336209197\n",
      "INFO:__main__:[test] Filtered Mean Rank: 381.25540336209195\n",
      "INFO:__main__:[test] Filtered Hits@1: 55.36333718758338\n",
      "INFO:__main__:[test] Filtered Hits@3: 65.94770079160367\n",
      "INFO:__main__:[test] Filtered Hits@5: 68.91843813928666\n",
      "INFO:__main__:[test] Filtered Hits@10: 71.59565952148003\n"
     ]
    }
   ],
   "source": [
    "for eval_name, eval_triples in [('valid', valid_triples), ('test', test_triples)]:\n",
    "\n",
    "    ranks_subj, ranks_obj = [], []\n",
    "    filtered_ranks_subj, filtered_ranks_obj = [], []\n",
    "\n",
    "    for _i, (s, p, o) in enumerate(eval_triples):\n",
    "        s_idx, p_idx, o_idx = entity_to_idx[s], predicate_to_idx[p], entity_to_idx[o]\n",
    "\n",
    "        Xs = np.full(shape=(nb_entities,), fill_value=s_idx, dtype=np.int32)\n",
    "        Xp = np.full(shape=(nb_entities,), fill_value=p_idx, dtype=np.int32)\n",
    "        Xo = np.full(shape=(nb_entities,), fill_value=o_idx, dtype=np.int32)\n",
    "\n",
    "        feed_dict_corrupt_subj = {subject_inputs: np.arange(nb_entities), predicate_inputs: Xp, object_inputs: Xo}\n",
    "        feed_dict_corrupt_obj = {subject_inputs: Xs, predicate_inputs: Xp, object_inputs: np.arange(nb_entities)}\n",
    "\n",
    "        # scores of (1, p, o), (2, p, o), .., (N, p, o)\n",
    "        scores_subj = session.run(scores, feed_dict=feed_dict_corrupt_subj)\n",
    "\n",
    "        # scores of (s, p, 1), (s, p, 2), .., (s, p, N)\n",
    "        scores_obj = session.run(scores, feed_dict=feed_dict_corrupt_obj)\n",
    "\n",
    "        ranks_subj += [1 + np.sum(scores_subj > scores_subj[s_idx])]\n",
    "        ranks_obj += [1 + np.sum(scores_obj > scores_obj[o_idx])]\n",
    "\n",
    "        filtered_scores_subj = scores_subj.copy()\n",
    "        filtered_scores_obj = scores_obj.copy()\n",
    "\n",
    "        rm_idx_s = [entity_to_idx[fs] for (fs, fp, fo) in all_triples if fs != s and fp == p and fo == o]\n",
    "        rm_idx_o = [entity_to_idx[fo] for (fs, fp, fo) in all_triples if fs == s and fp == p and fo != o]\n",
    "\n",
    "        filtered_scores_subj[rm_idx_s] = - np.inf\n",
    "        filtered_scores_obj[rm_idx_o] = - np.inf\n",
    "\n",
    "        filtered_ranks_subj += [1 + np.sum(filtered_scores_subj > filtered_scores_subj[s_idx])]\n",
    "        filtered_ranks_obj += [1 + np.sum(filtered_scores_obj > filtered_scores_obj[o_idx])]\n",
    "\n",
    "        if _i % 1000 == 0:\n",
    "            logger.info('{}/{} ..'.format(_i, len(eval_triples)))\n",
    "        \n",
    "        \n",
    "    ranks = ranks_subj + ranks_obj\n",
    "    filtered_ranks = filtered_ranks_subj + filtered_ranks_obj\n",
    "\n",
    "    for setting_name, setting_ranks in [('Raw', ranks), ('Filtered', filtered_ranks)]:\n",
    "        mean_rank = np.mean(setting_ranks)\n",
    "        logger.info('[{}] {} Mean Rank: {}'.format(eval_name, setting_name, mean_rank))\n",
    "        for k in [1, 3, 5, 10]:\n",
    "            hits_at_k = np.mean(np.asarray(setting_ranks) <= k) * 100\n",
    "            logger.info('[{}] {} Hits@{}: {}'.format(eval_name, setting_name, k, hits_at_k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
