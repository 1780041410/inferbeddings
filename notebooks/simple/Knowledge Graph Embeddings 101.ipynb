{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def read_triples(path):\n",
    "    triples = []\n",
    "    with open(path, 'rt') as f:\n",
    "        for line in f.readlines():\n",
    "            s, p, o = line.split()\n",
    "            triples += [(s.strip(), p.strip(), o.strip())]\n",
    "    return triples\n",
    "\n",
    "\n",
    "def unit_cube_projection(var_matrix):\n",
    "    unit_cube_projection = tf.minimum(1., tf.maximum(var_matrix, 0.))\n",
    "    return tf.assign(var_matrix, unit_cube_projection)\n",
    "\n",
    "\n",
    "def make_batches(size, batch_size):\n",
    "    nb_batch = int(np.ceil(size / float(batch_size)))\n",
    "    res = [(i * batch_size, min(size, (i + 1) * batch_size)) for i in range(0, nb_batch)]\n",
    "    return res\n",
    "\n",
    "class IndexGenerator:\n",
    "    def __init__(self):\n",
    "        self.random_state = np.random.RandomState(0)\n",
    "\n",
    "    def __call__(self, n_samples, candidate_indices):\n",
    "        shuffled_indices = candidate_indices[self.random_state.permutation(len(candidate_indices))]\n",
    "        rand_ints = shuffled_indices[np.arange(n_samples) % len(shuffled_indices)]\n",
    "        return rand_ints\n",
    "\n",
    "class DistMult:\n",
    "    def __init__(self, subject_embeddings=None, object_embeddings=None,\n",
    "                 predicate_embeddings=None,):\n",
    "        self.subject_embeddings, self.object_embeddings = subject_embeddings, object_embeddings\n",
    "        self.predicate_embeddings = predicate_embeddings\n",
    "\n",
    "    def __call__(self):\n",
    "        scores = tf.reduce_sum(self.subject_embeddings *\n",
    "                               self.predicate_embeddings *\n",
    "                               self.object_embeddings, axis=1)\n",
    "        return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "entity_embedding_size = 150\n",
    "predicate_embedding_size = 150\n",
    "\n",
    "seed = 0\n",
    "margin = 5\n",
    "\n",
    "nb_epochs = 1000\n",
    "nb_batches = 10\n",
    "\n",
    "np.random.seed(seed)\n",
    "random_state = np.random.RandomState(seed)\n",
    "tf.set_random_seed(seed)\n",
    "\n",
    "dataset_name = 'wn18'\n",
    "\n",
    "train_triples = read_triples('{}/{}.train.tsv'.format(dataset_name, dataset_name))\n",
    "valid_triples = read_triples('{}/{}.valid.tsv'.format(dataset_name, dataset_name))\n",
    "test_triples = read_triples('{}/{}.test.tsv'.format(dataset_name, dataset_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_triples = train_triples + valid_triples + test_triples\n",
    "\n",
    "entity_set = {s for (s, p, o) in all_triples} | {o for (s, p, o) in all_triples}\n",
    "predicate_set = {p for (s, p, o) in all_triples}\n",
    "\n",
    "nb_entities, nb_predicates = len(entity_set), len(predicate_set)\n",
    "nb_examples = len(train_triples)\n",
    "\n",
    "entity_to_idx = {entity: idx for idx, entity in enumerate(sorted(entity_set))}\n",
    "predicate_to_idx = {predicate: idx for idx, predicate in enumerate(sorted(predicate_set))}\n",
    "\n",
    "entity_embedding_layer = tf.get_variable('entities', shape=[nb_entities, entity_embedding_size],\n",
    "                                         initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "predicate_embedding_layer = tf.get_variable('predicates', shape=[nb_predicates, predicate_embedding_size],\n",
    "                                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "subject_inputs = tf.placeholder(tf.int32, shape=[None])\n",
    "predicate_inputs = tf.placeholder(tf.int32, shape=[None])\n",
    "object_inputs = tf.placeholder(tf.int32, shape=[None])\n",
    "\n",
    "target_inputs = tf.placeholder(tf.float32, shape=[None])\n",
    "\n",
    "subject_embeddings = tf.nn.embedding_lookup(entity_embedding_layer, subject_inputs)\n",
    "predicate_embeddings = tf.nn.embedding_lookup(predicate_embedding_layer, predicate_inputs)\n",
    "object_embeddings = tf.nn.embedding_lookup(entity_embedding_layer, object_inputs)\n",
    "\n",
    "model = DistMult(subject_embeddings=subject_embeddings,\n",
    "                 predicate_embeddings=predicate_embeddings,\n",
    "                 object_embeddings=object_embeddings)\n",
    "\n",
    "scores = model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "hinge_losses = tf.nn.relu(margin - scores * (2 * target_inputs - 1))\n",
    "loss = tf.reduce_sum(hinge_losses)\n",
    "\n",
    "optimizer = tf.train.AdagradOptimizer(learning_rate=0.1)\n",
    "training_step = optimizer.minimize(loss)\n",
    "\n",
    "projection_step = unit_cube_projection(entity_embedding_layer)\n",
    "\n",
    "batch_size = math.ceil(nb_examples / nb_batches)\n",
    "batches = make_batches(nb_examples, batch_size)\n",
    "\n",
    "nb_versions = 3\n",
    "\n",
    "Xs = np.array([entity_to_idx[s] for (s, p, o) in train_triples], dtype=np.int32)\n",
    "Xp = np.array([predicate_to_idx[p] for (s, p, o) in train_triples], dtype=np.int32)\n",
    "Xo = np.array([entity_to_idx[o] for (s, p, o) in train_triples], dtype=np.int32)\n",
    "\n",
    "index_gen = IndexGenerator()\n",
    "\n",
    "init_op = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Epoch 10\tLoss value: 5.4755 ± 0.1380\n",
      "INFO:__main__:Epoch 20\tLoss value: 0.2603 ± 0.0061\n",
      "INFO:__main__:Epoch 30\tLoss value: 0.0730 ± 0.0038\n",
      "INFO:__main__:Epoch 40\tLoss value: 0.0492 ± 0.0039\n",
      "INFO:__main__:Epoch 50\tLoss value: 0.0430 ± 0.0032\n",
      "INFO:__main__:Epoch 60\tLoss value: 0.0363 ± 0.0037\n"
     ]
    }
   ],
   "source": [
    "def stats(values):\n",
    "    return '{0:.4f} ± {1:.4f}'.format(round(np.mean(values), 4), round(np.std(values), 4))\n",
    "\n",
    "session = tf.Session()\n",
    "session.run(init_op)\n",
    "\n",
    "for epoch in range(1, nb_epochs + 1):\n",
    "    order = random_state.permutation(nb_examples)\n",
    "    Xs_shuf, Xp_shuf, Xo_shuf = Xs[order], Xp[order], Xo[order]\n",
    "    \n",
    "    loss_values = []\n",
    "\n",
    "    for batch_no, (batch_start, batch_end) in enumerate(batches):\n",
    "        curr_batch_size = batch_end - batch_start\n",
    "\n",
    "        Xs_batch = np.zeros(curr_batch_size * nb_versions, dtype=Xs_shuf.dtype)\n",
    "        Xp_batch = np.zeros(curr_batch_size * nb_versions, dtype=Xp_shuf.dtype)\n",
    "        Xo_batch = np.zeros(curr_batch_size * nb_versions, dtype=Xo_shuf.dtype)\n",
    "\n",
    "        Xs_batch[0::nb_versions] = Xs_shuf[batch_start:batch_end]\n",
    "        Xp_batch[0::nb_versions] = Xp_shuf[batch_start:batch_end]\n",
    "        Xo_batch[0::nb_versions] = Xo_shuf[batch_start:batch_end]\n",
    "\n",
    "        # Xs_batch[1::nb_versions] needs to be corrupted\n",
    "        Xs_batch[1::nb_versions] = index_gen(curr_batch_size, np.arange(nb_entities))\n",
    "        Xp_batch[1::nb_versions] = Xp_shuf[batch_start:batch_end]\n",
    "        Xo_batch[1::nb_versions] = Xo_shuf[batch_start:batch_end]\n",
    "\n",
    "        # Xo_batch[2::nb_versions] needs to be corrupted\n",
    "        Xs_batch[2::nb_versions] = Xs_shuf[batch_start:batch_end]\n",
    "        Xp_batch[2::nb_versions] = Xp_shuf[batch_start:batch_end]\n",
    "        Xo_batch[2::nb_versions] = index_gen(curr_batch_size, np.arange(nb_entities))\n",
    "\n",
    "        feed_dict = {\n",
    "            subject_inputs: Xs_batch, predicate_inputs: Xp_batch, object_inputs: Xo_batch,\n",
    "            target_inputs: np.array([1.0, 0.0, 0.0] * curr_batch_size)\n",
    "        }\n",
    "\n",
    "        _, loss_value = session.run([training_step, loss], feed_dict=feed_dict)\n",
    "        session.run(projection_step)\n",
    "\n",
    "        loss_values += [loss_value / (Xp_batch.shape[0] / nb_versions)]\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        logger.info('Epoch {0}\\tLoss value: {1}'.format(epoch, stats(loss_values)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for eval_name, eval_triples in [('valid', valid_triples), ('test', test_triples)]:\n",
    "\n",
    "    ranks_subj, ranks_obj = [], []\n",
    "    filtered_ranks_subj, filtered_ranks_obj = [], []\n",
    "\n",
    "    for _i, (s, p, o) in enumerate(eval_triples):\n",
    "        s_idx, p_idx, o_idx = entity_to_idx[s], predicate_to_idx[p], entity_to_idx[o]\n",
    "\n",
    "        Xs = np.full(shape=(nb_entities,), fill_value=s_idx, dtype=np.int32)\n",
    "        Xp = np.full(shape=(nb_entities,), fill_value=p_idx, dtype=np.int32)\n",
    "        Xo = np.full(shape=(nb_entities,), fill_value=o_idx, dtype=np.int32)\n",
    "\n",
    "        feed_dict_corrupt_subj = {subject_inputs: np.arange(nb_entities), predicate_inputs: Xp, object_inputs: Xo}\n",
    "        feed_dict_corrupt_obj = {subject_inputs: Xs, predicate_inputs: Xp, object_inputs: np.arange(nb_entities)}\n",
    "\n",
    "        # scores of (1, p, o), (2, p, o), .., (N, p, o)\n",
    "        scores_subj = session.run(scores, feed_dict=feed_dict_corrupt_subj)\n",
    "\n",
    "        # scores of (s, p, 1), (s, p, 2), .., (s, p, N)\n",
    "        scores_obj = session.run(scores, feed_dict=feed_dict_corrupt_obj)\n",
    "\n",
    "        ranks_subj += [1 + np.sum(scores_subj > scores_subj[s_idx])]\n",
    "        ranks_obj += [1 + np.sum(scores_obj > scores_obj[o_idx])]\n",
    "\n",
    "        filtered_scores_subj = scores_subj.copy()\n",
    "        filtered_scores_obj = scores_obj.copy()\n",
    "\n",
    "        rm_idx_s = [entity_to_idx[fs] for (fs, fp, fo) in all_triples if fs != s and fp == p and fo == o]\n",
    "        rm_idx_o = [entity_to_idx[fo] for (fs, fp, fo) in all_triples if fs == s and fp == p and fo != o]\n",
    "\n",
    "        filtered_scores_subj[rm_idx_s] = - np.inf\n",
    "        filtered_scores_obj[rm_idx_o] = - np.inf\n",
    "\n",
    "        filtered_ranks_subj += [1 + np.sum(filtered_scores_subj > filtered_scores_subj[s_idx])]\n",
    "        filtered_ranks_obj += [1 + np.sum(filtered_scores_obj > filtered_scores_obj[o_idx])]\n",
    "\n",
    "        if _i % 1000 == 0:\n",
    "            logger.info('{}/{} ..'.format(_i, len(eval_triples)))\n",
    "        \n",
    "        \n",
    "    ranks = ranks_subj + ranks_obj\n",
    "    filtered_ranks = filtered_ranks_subj + filtered_ranks_obj\n",
    "\n",
    "    for setting_name, setting_ranks in [('Raw', ranks), ('Filtered', filtered_ranks)]:\n",
    "        mean_rank = np.mean(setting_ranks)\n",
    "        logger.info('[{}] {} Mean Rank: {}'.format(eval_name, setting_name, mean_rank))\n",
    "        for k in [1, 3, 5, 10]:\n",
    "            hits_at_k = np.mean(np.asarray(setting_ranks) <= k) * 100\n",
    "            logger.info('[{}] {} Hits@{}: {}'.format(eval_name, setting_name, k, hits_at_k))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
